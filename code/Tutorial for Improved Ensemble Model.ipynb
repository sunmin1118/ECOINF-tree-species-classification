{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4b7938-86d8-4688-9483-1a8bfcddcab5",
   "metadata": {},
   "source": [
    "#  Tutorial for Improved Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab5506-3d71-4181-af39-51171d020263",
   "metadata": {},
   "source": [
    "## 1. Activate Local Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240606e5-f390-4ff3-b4db-401d7284da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e91c2c-25a8-4b07-9d6e-33f1455e8c5f",
   "metadata": {},
   "source": [
    "## 2. Base Classifiers Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d13bb-329c-4c8c-b392-453e5277df9b",
   "metadata": {},
   "source": [
    "###  2.1 ML classifiers modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a557ddd-569b-4454-b94d-31391f5489c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import joblib  # Add joblib for model serialization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71a7950f-6fd4-4a58-83a7-1f9658065057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Completed.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Data Processing\n",
    "class ShadowDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(train_path, val_path):\n",
    "    os.makedirs(r'venv\\best_parameter_MLmodel', exist_ok=True)\n",
    "    os.makedirs(r'venv', exist_ok=True)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_targets = pd.concat([train_df['Target'], val_df['Target']]) #Target: original dependent variable\n",
    "    label_encoder.fit(all_targets)\n",
    "    train_df['Target_Encoded'] = label_encoder.transform(train_df['Target'])\n",
    "    val_df['Target_Encoded'] = label_encoder.transform(val_df['Target'])\n",
    "    joblib.dump(label_encoder, r'venv\\MLmodel\\label_encoder.joblib')\n",
    "\n",
    "    # Separate features and labels\n",
    "    X_train = train_df.drop(['Target', 'Target_Encoded'], axis=1).values\n",
    "    y_train = train_df['Target_Encoded'].values\n",
    "    X_val = val_df.drop(['Target', 'Target_Encoded'], axis=1).values\n",
    "    y_val = val_df['Target_Encoded'].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    joblib.dump(scaler, r'venv\\MLmodel\\scaler.joblib')\n",
    "    print(f\"Data Preprocessing Completed！\")\n",
    "    \n",
    "    # Initialize model dictionary\n",
    "    models = {\n",
    "        'MLP': None,\n",
    "        'RF': None,\n",
    "        'SVM': None\n",
    "    }\n",
    "\n",
    "\n",
    "    input_size = X_train_scaled.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    train_dataset = ShadowDataset(np.expand_dims(X_train_scaled, axis=1), y_train)\n",
    "    val_dataset = ShadowDataset(np.expand_dims(X_val_scaled, axis=1), y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # MLP\n",
    "    mlp_params = {\n",
    "        'hidden_layer_sizes': [(50, 50), (100,), (50, 100, 50)],\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['adam'],\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "    mlp = MLPClassifier(random_state=42)\n",
    "    mlp_grid = GridSearchCV(mlp, mlp_params, cv=5, scoring='f1_weighted')\n",
    "    mlp_grid.fit(X_train_scaled, y_train)\n",
    "    models['MLP'] = mlp_grid.best_estimator_\n",
    "\n",
    "    joblib.dump(mlp_grid.best_estimator_, r'venv\\MLmodel\\best_mlp_model.joblib')\n",
    "\n",
    "    # RF\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1_weighted')\n",
    "    rf_grid.fit(X_train_scaled, y_train)\n",
    "    models['RF'] = rf_grid.best_estimator_\n",
    "    joblib.dump(rf_grid.best_estimator_, r'venv\\MLmodel\\best_rf_model.joblib')\n",
    "\n",
    "    # SVM\n",
    "    svm_params = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    svm = SVC(random_state=42, probability=True)\n",
    "    svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='f1_weighted')\n",
    "    svm_grid.fit(X_train_scaled, y_train)\n",
    "    models['SVM'] = svm_grid.best_estimator_\n",
    "    joblib.dump(svm_grid.best_estimator_, r'venv\\MLmodel\\best_svm_model.joblib')\n",
    "\n",
    "    print(\"Model Evaluation Completed.\")\n",
    "    # Prepare a dictionary to store results\n",
    "    f1_scores = {}\n",
    "    confusion_matrices = {}\n",
    "    f1_scores_per_class = {}\n",
    "\n",
    "    # Evaluate All Models\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        y_true = y_val\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        \n",
    "        f1_per_class = f1_score(\n",
    "        y_true, \n",
    "        y_pred, \n",
    "        average=None, \n",
    "        labels=np.unique(y_true))\n",
    "        f1_scores_per_class[name] = f1_per_class\n",
    "\n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'OA': accuracy,  # OA is same as accuracy in classification\n",
    "            'F1_score': f1,\n",
    "            'Classification_report': report\n",
    "        }\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        confusion_matrices[name] = cm\n",
    "        cm_df = pd.DataFrame(cm)\n",
    "        with open(r'venv\\validation\\MLmodelcm.csv', 'a') as f:\n",
    "            f.write(f\"\\n{name} Confusion Matrix:\\n\")\n",
    "        cm_df.to_csv(r'venv\\validation\\MLmodelcm.csv', mode='a', header=False)\n",
    "        f1_scores[name] = classification_report(y_true, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "        \n",
    "    f1_df = pd.DataFrame.from_dict(f1_scores_per_class, orient='index', columns=[f'Class_{i}' for i in range(len(f1_scores_per_class['MLP']))])\n",
    "    f1_df.index.name = 'Model'\n",
    "    f1_df.to_csv(r'venv\\validation\\F1_3MLmodel.csv')\n",
    "\n",
    "\n",
    "train_and_evaluate_models(\n",
    "    r'venv\\dataset\\firstlayertrain.csv',\n",
    "    r'venv\\dataset\\firstlayerval.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b76c3-6a0b-4016-ae2b-37396f82ab93",
   "metadata": {},
   "source": [
    "###  2.2 DL classifiers modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b4568-12c0-4b59-b27b-52790d9dcfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87a2bb-28bf-4c8d-a3bb-927eebafd713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShadowDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def train_model(train_file_path, val_file_path):\n",
    "    train_df = pd.read_csv(train_file_path)\n",
    "    val_df = pd.read_csv(val_file_path)\n",
    "\n",
    "    # LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df['Target_Encoded'] = label_encoder.fit_transform(train_df['Target'])\n",
    "    val_df['Target_Encoded'] = label_encoder.transform(val_df['Target'])\n",
    "\n",
    "    X_train = train_df.drop(['Target', 'Target_Encoded'], axis=1).values\n",
    "    y_train = train_df['Target_Encoded'].values\n",
    "    X_val = val_df.drop(['Target', 'Target_Encoded'], axis=1).values\n",
    "    y_val = val_df['Target_Encoded'].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    def reshape_for_model(data):\n",
    "        return np.expand_dims(data, axis=1)\n",
    "\n",
    "    X_train_reshaped = reshape_for_model(X_train_scaled)\n",
    "    X_val_reshaped = reshape_for_model(X_val_scaled)\n",
    "\n",
    "    train_dataset = ShadowDataset(X_train_reshaped, y_train)\n",
    "    val_dataset = ShadowDataset(X_val_reshaped, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y_train))  \n",
    "\n",
    "    class EnhancedAttentionCNN(nn.Module):\n",
    "        def __init__(self, input_size, num_classes):\n",
    "            super(EnhancedAttentionCNN, self).__init__()\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(input_size, input_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(input_size, input_size),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_input = torch.zeros(1, 1, input_size)\n",
    "                feature_size = self._calculate_feature_size(test_input)\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(256 * feature_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(256, num_classes)\n",
    "            )\n",
    "\n",
    "        def _calculate_feature_size(self, x):\n",
    "            return self.features(x).size(2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x_flat = x.view(x.size(0), -1)\n",
    "            attention_weights = self.attention(x_flat)\n",
    "            x_attended = x_flat * attention_weights\n",
    "\n",
    "            x_conv = x_attended.view(x.size(0), 1, -1)\n",
    "\n",
    "            features = self.features(x_conv)\n",
    "\n",
    "            x = features.view(features.size(0), -1)\n",
    "\n",
    "            output = self.classifier(x)\n",
    "\n",
    "            return output, attention_weights\n",
    "\n",
    "    model = EnhancedAttentionCNN(input_size, num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)\n",
    "\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    num_epochs = 100  \n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train_samples += batch_labels.size(0)\n",
    "            total_train_correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_features, val_labels in val_loader:\n",
    "                outputs, _ = model(val_features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val_samples += val_labels.size(0)\n",
    "                total_val_correct += (predicted == val_labels).sum().item()\n",
    "                y_pred_list.extend(predicted.numpy())\n",
    "                y_true_list.extend(val_labels.numpy())\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "\n",
    "            os.makedirs('venv/MLmodel', exist_ok=True)\n",
    "            torch.save(model.state_dict(), 'venv/MLmodel/Best_CNN_Model.pth')\n",
    "\n",
    "        # print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        # print(f'Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "        # print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    training_history_df = pd.DataFrame({\n",
    "        'Epoch': range(1, num_epochs + 1),\n",
    "        'Train_Loss': train_losses,\n",
    "        'Train_Accuracy': train_accuracies,\n",
    "        'Validation_Accuracy': val_accuracies\n",
    "    })\n",
    "    training_history_df.to_csv('venv/CNN epoch.csv', index=False)\n",
    "\n",
    "    model.load_state_dict(torch.load('venv/MLmodel/Best_CNN_Model.pth'))\n",
    "\n",
    "    train_attention_weights = []\n",
    "    val_attention_weights = []\n",
    "    train_attention_weighted_features = []\n",
    "    val_attention_weighted_features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for train_features, train_labels in train_loader:\n",
    "            _, batch_attention = model(train_features)\n",
    "            train_attention_weights.extend(batch_attention.numpy())\n",
    "\n",
    "            x_flat = train_features.view(train_features.size(0), -1)\n",
    "            x_attended = x_flat * batch_attention\n",
    "            train_attention_weighted_features.extend(x_attended.numpy())\n",
    "\n",
    "        for val_features, val_labels in val_loader:\n",
    "            _, batch_attention = model(val_features)\n",
    "            val_attention_weights.extend(batch_attention.numpy())\n",
    "\n",
    "            x_flat = val_features.view(val_features.size(0), -1)\n",
    "            x_attended = x_flat * batch_attention\n",
    "            val_attention_weighted_features.extend(x_attended.numpy())\n",
    "\n",
    "    train_attention_df = pd.DataFrame(train_attention_weights,\n",
    "                                      columns=[f'Feature_{i}' for i in range(len(train_attention_weights[0]))])\n",
    "    val_attention_df = pd.DataFrame(val_attention_weights,\n",
    "                                    columns=[f'Feature_{i}' for i in range(len(val_attention_weights[0]))])\n",
    "\n",
    "    os.makedirs('venv/dataset', exist_ok=True)\n",
    "\n",
    "    train_attention_df.to_csv('venv/dataset/weight_train.csv', index=False)\n",
    "    val_attention_df.to_csv('venv/dataset/weight_val.csv', index=False)\n",
    "\n",
    "    train_attention_weighted_df = pd.DataFrame(train_attention_weighted_features,\n",
    "                                               columns=[f'Weighted_Feature_{i}' for i in\n",
    "                                                        range(len(train_attention_weighted_features[0]))])\n",
    "    train_attention_weighted_df['Target'] = train_df['Target']  \n",
    "\n",
    "    val_attention_weighted_df = pd.DataFrame(val_attention_weighted_features,\n",
    "                                             columns=[f'Weighted_Feature_{i}' for i in\n",
    "                                                      range(len(val_attention_weighted_features[0]))])\n",
    "    val_attention_weighted_df['Target'] = val_df['Target']  \n",
    "\n",
    "    train_attention_weighted_df.to_csv('venv/dataset/weighted_train.csv', index=False)\n",
    "    val_attention_weighted_df.to_csv('venv/dataset/weighted_val.csv', index=False)\n",
    "\n",
    "    cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    print(\"classification report\")\n",
    "    print(classification_report(y_true_list, y_pred_list))\n",
    "\n",
    "    per_class_f1 = f1_score(y_true_list, y_pred_list, average=None)\n",
    "    f1_df = pd.DataFrame(per_class_f1, columns=[\"CNN\"])\n",
    "    f1_df.to_csv('venv/validation/CNN_F1_scores.csv', index=False)\n",
    "    print(\"F1 saved at: venv/validation/CNN_F1_scores.csv\")\n",
    "    return best_accuracy,model\n",
    "\n",
    "\n",
    "accuracy, trained_model = train_model(r'venv\\dataset\\firstlayertrain.csv', r'venv\\dataset\\firstlayerval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259b69e-1699-42e0-aea9-b883c58a3780",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Dictionary generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd379f80-844f-4c41-9e33-2b18e2b794b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_path = 'venv/validation/CNN_F1_scores.csv'\n",
    "mlmodels_path = 'venv/validation/F1_3MLmodel.csv'\n",
    "output_path = 'venv/validation/F1_dictionary.csv'\n",
    "\n",
    "with open(cnn_path, 'r') as f:\n",
    "    cnn_values = [float(line.strip()) for line in f.readlines()[1:]]\n",
    "\n",
    "mlmodels_df = pd.read_csv(mlmodels_path, header=None, skiprows=1)  \n",
    "mlmodels_df.columns = ['Model'] + [f'Category_{i}' for i in range(1, 9)]\n",
    "\n",
    "data = {\n",
    "    'MLP': mlmodels_df[mlmodels_df['Model'] == 'MLP'].iloc[0, 1:].tolist(),\n",
    "    'RF': mlmodels_df[mlmodels_df['Model'] == 'RF'].iloc[0, 1:].tolist(),\n",
    "    'SVM': mlmodels_df[mlmodels_df['Model'] == 'SVM'].iloc[0, 1:].tolist(),\n",
    "    'CNN': cnn_values\n",
    "}\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "\n",
    "result_df.to_csv(output_path, header=True, index=False)\n",
    "\n",
    "print(f\"weighted dictionary saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67ba45-28f9-4835-b8d8-9f745e6d2540",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Weighted features generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528fb95-f4f2-4587-9fbb-c957e996a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import joblib\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "class EnhancedAttentionCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(EnhancedAttentionCNN, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 1, input_size)\n",
    "            feature_size = self._calculate_feature_size(test_input)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _calculate_feature_size(self, x):\n",
    "        return self.features(x).size(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        attention_weights = self.attention(x_flat)\n",
    "        x_attended = x_flat * attention_weights\n",
    "        x_conv = x_attended.view(x.size(0), 1, -1)\n",
    "\n",
    "        features = self.features(x_conv)\n",
    "\n",
    "        x = features.view(features.size(0), -1)\n",
    "\n",
    "        output = self.classifier(x)\n",
    "\n",
    "        return output, attention_weights, x_attended\n",
    "\n",
    "\n",
    "def process_models_and_probabilities(train_path):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    label_encoder = joblib.load(r'venv\\MLmodel\\label_encoder.joblib')\n",
    "    scaler = joblib.load(r'venv\\MLmodel\\scaler.joblib')\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    train_df['Target_Encoded'] = label_encoder.transform(train_df['Target'])\n",
    "    \n",
    "    X_train = train_df.drop(['Target', 'Target_Encoded'], axis=1).values\n",
    "    y_train = train_df['Target_Encoded'].values\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    # Load ML model\n",
    "    mlp_model = joblib.load(r'venv\\MLmodel\\best_mlp_model.joblib')\n",
    "    rf_model = joblib.load(r'venv\\MLmodel\\best_rf_model.joblib')\n",
    "    svm_model = joblib.load(r'venv\\MLmodel\\best_svm_model.joblib')\n",
    "\n",
    "    # load 1DCNN model\n",
    "    cnn_model = EnhancedAttentionCNN(input_size, num_classes)\n",
    "    cnn_model.load_state_dict(torch.load(r'venv\\MLmodel\\Best_CNN_Model.pth'))\n",
    "    cnn_model.eval()\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "    # Load weighted dictionary\n",
    "    f1_dict_path = r'venv\\validation\\F1_dictionary.csv'\n",
    "    f1_dict_df = pd.read_csv(f1_dict_path)\n",
    "\n",
    "    # MLP\n",
    "    mlp_proba = mlp_model.predict_proba(X_train_scaled)\n",
    "\n",
    "    # RF\n",
    "    rf_proba = rf_model.predict_proba(X_train_scaled)\n",
    "\n",
    "    # SVM\n",
    "    svm_proba = svm_model.predict_proba(X_train_scaled)\n",
    "\n",
    "\n",
    "    # 1DCNN\n",
    "    X_train_tensor = torch.FloatTensor(np.expand_dims(X_train_scaled, axis=1))\n",
    "    with torch.no_grad():\n",
    "        cnn_outputs, _, cnn_attended_features = cnn_model(X_train_tensor)\n",
    "        cnn_proba = torch.softmax(cnn_outputs, dim=1).numpy()\n",
    "\n",
    "    cnn_pred_labels = torch.argmax(cnn_outputs, dim=1).numpy()\n",
    "\n",
    "    weighted_probas = []\n",
    "    for i in range(len(X_train_scaled)):\n",
    "        mlp_pred = np.argmax(mlp_proba[i])\n",
    "        rf_pred = np.argmax(rf_proba[i])\n",
    "        svm_pred = np.argmax(svm_proba[i])\n",
    "        cnn_pred = np.argmax(cnn_proba[i])\n",
    "        f1_mlp = f1_dict_df.loc[mlp_pred, 'MLP']\n",
    "        f1_rf = f1_dict_df.loc[rf_pred, 'RF']\n",
    "        f1_svm = f1_dict_df.loc[svm_pred, 'SVM']\n",
    "        f1_cnn = f1_dict_df.loc[cnn_pred, 'CNN']\n",
    "\n",
    "\n",
    "        # 加权概率计算\n",
    "        weighted_proba = (\n",
    "                mlp_proba[i] * f1_mlp +\n",
    "                rf_proba[i] * f1_rf +\n",
    "                svm_proba[i] * f1_svm +\n",
    "                cnn_proba[i] * f1_cnn\n",
    "        )\n",
    "\n",
    "        weighted_probas.append(weighted_proba)\n",
    "\n",
    "    weighted_probas = np.array(weighted_probas)\n",
    "\n",
    "    output_df = pd.DataFrame(weighted_probas, columns=[f'Proba_{cls}' for cls in label_encoder.classes_])\n",
    "    output_df.insert(0, 'Target', train_df['Target'])\n",
    "\n",
    "    attended_features_df = pd.DataFrame(cnn_attended_features.numpy())\n",
    "    attended_features_df.insert(0, 'Target', train_df['Target'])\n",
    "\n",
    "    final_df = pd.concat([output_df, attended_features_df.iloc[:, 1:].add_prefix('AttendedFeature_')], axis=1)\n",
    "\n",
    "\n",
    "    # save results\n",
    "    output_path = r'venv\\dataset\\firstlayeroutput_weighted_probabilities_train.csv'\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Weighted probabilities and attention features have been saved to {output_path}\")\n",
    "\n",
    "process_models_and_probabilities(\n",
    "    r'venv\\dataset\\firstlayertrain.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d12177-dc8c-4dfe-b16e-c3ea9ed4fe70",
   "metadata": {},
   "source": [
    "### 2.5 Validation dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477913db-fceb-468a-85e1-2538345f4cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnhancedAttentionCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(EnhancedAttentionCNN, self).__init__()\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 1, input_size)\n",
    "            feature_size = self._calculate_feature_size(test_input)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _calculate_feature_size(self, x):\n",
    "        return self.features(x).size(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        attention_weights = self.attention(x_flat)\n",
    "        x_attended = x_flat * attention_weights\n",
    "        x_conv = x_attended.view(x.size(0), 1, -1)\n",
    "        features = self.features(x_conv)\n",
    "        x = features.view(features.size(0), -1)\n",
    "        output = self.classifier(x)\n",
    "        return output, attention_weights, x_attended\n",
    "\n",
    "\n",
    "def process_models_and_probabilities(data_path):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    label_encoder = joblib.load(r'venv\\MLmodel\\label_encoder.joblib')\n",
    "    scaler = joblib.load(r'venv\\MLmodel\\scaler.joblib')\n",
    "\n",
    "    data_df = pd.read_csv(data_path)\n",
    "\n",
    "    data_df['Target_Encoded'] = label_encoder.transform(data_df['Target'])\n",
    "\n",
    "    X = data_df.drop(['Target', 'Target_Encoded'], axis=1).values\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    # Load ML models\n",
    "    mlp_model = joblib.load(r'venv\\MLmodel\\best_mlp_model.joblib')\n",
    "    rf_model = joblib.load(r'venv\\MLmodel\\best_rf_model.joblib')\n",
    "    svm_model = joblib.load(r'venv\\MLmodel\\best_svm_model.joblib')\n",
    "\n",
    "    # Load 1DCNN model\n",
    "    cnn_model = EnhancedAttentionCNN(input_size, num_classes)\n",
    "    cnn_model.load_state_dict(torch.load(r'venv\\MLmodel\\Best_CNN_Model.pth'))\n",
    "    cnn_model.eval()\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Load weighted dictionary\n",
    "    f1_dict_path = r'venv\\validation\\F1_dictionary.csv'\n",
    "    f1_dict_df = pd.read_csv(f1_dict_path)\n",
    "\n",
    "\n",
    "    # Retrieve the class probabilities of each model\n",
    "    # MLP\n",
    "    mlp_proba = mlp_model.predict_proba(X_scaled)\n",
    "\n",
    "    # RF\n",
    "    rf_proba = rf_model.predict_proba(X_scaled)\n",
    "\n",
    "    # SVM\n",
    "    svm_proba = svm_model.predict_proba(X_scaled)\n",
    "\n",
    "    # CNN\n",
    "    X_tensor = torch.FloatTensor(np.expand_dims(X_scaled, axis=1))\n",
    "    with torch.no_grad():\n",
    "        cnn_outputs, _, cnn_attended_features = cnn_model(X_tensor)\n",
    "        cnn_proba = torch.softmax(cnn_outputs, dim=1).numpy()\n",
    "\n",
    "    cnn_pred_labels = torch.argmax(cnn_outputs, dim=1).numpy()\n",
    "\n",
    "    weighted_probas = []\n",
    "    for i in range(len(X_scaled)):\n",
    "        # Obtain the predicted classes of each model\n",
    "        mlp_pred = np.argmax(mlp_proba[i])\n",
    "        rf_pred = np.argmax(rf_proba[i])\n",
    "        svm_pred = np.argmax(svm_proba[i])\n",
    "        cnn_pred = np.argmax(cnn_proba[i])\n",
    "\n",
    "        # Obtain the F1 score of each model for its own predicted class\n",
    "        f1_mlp = f1_dict_df.loc[mlp_pred, 'MLP']\n",
    "        f1_rf = f1_dict_df.loc[rf_pred, 'RF']\n",
    "        f1_svm = f1_dict_df.loc[svm_pred, 'SVM']\n",
    "        f1_cnn = f1_dict_df.loc[cnn_pred, 'CNN']\n",
    "\n",
    "        # Weighted probability calculation\n",
    "        weighted_proba = (\n",
    "                mlp_proba[i] * f1_mlp +\n",
    "                rf_proba[i] * f1_rf +\n",
    "                svm_proba[i] * f1_svm +\n",
    "                cnn_proba[i] * f1_cnn\n",
    "        )\n",
    "\n",
    "\n",
    "        weighted_probas.append(weighted_proba)\n",
    "\n",
    "    weighted_probas = np.array(weighted_probas)\n",
    "\n",
    "    output_df = pd.DataFrame(weighted_probas, columns=[f'Proba_{cls}' for cls in label_encoder.classes_])\n",
    "    output_df.insert(0, 'Target', data_df['Target'])\n",
    "\n",
    "\n",
    "    attended_features_df = pd.DataFrame(cnn_attended_features.numpy())\n",
    "    attended_features_df.insert(0, 'Target', data_df['Target'])\n",
    "\n",
    "    final_df = pd.concat([output_df, attended_features_df.iloc[:, 1:].add_prefix('AttendedFeature_')], axis=1)\n",
    "\n",
    "    # Save outputs\n",
    "    output_path = r'venv\\dataset\\firstlayeroutput_weighted_probabilities_val.csv'\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Weighted probabilities and attention features have been saved to {output_path}\")\n",
    "\n",
    "process_models_and_probabilities(\n",
    "    r'venv\\dataset\\firstlayerval.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea729c-62a8-43c5-965e-105d4f59f0f2",
   "metadata": {},
   "source": [
    "## 3. Model Ensemble and Result Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98e4ec-d891-4ebe-b932-91b444aec45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346b5f5-a2bd-40f1-af83-608d55b8dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weighted_data(weighted_train_path, weighted_val_path):\n",
    "    weighted_train = pd.read_csv(weighted_train_path)\n",
    "    weighted_val = pd.read_csv(weighted_val_path)\n",
    "    return weighted_train, weighted_val\n",
    "\n",
    "def preprocess_data(train, val):\n",
    "    # Separate features and labels\n",
    "    X_train = train.drop(columns=['Target'])\n",
    "    y_train = train['Target']\n",
    "    X_val = val.drop(columns=['Target'])\n",
    "    y_val = val['Target']\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def perform_grid_search(X_train, y_train):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [20, 30, 40],\n",
    "        'min_samples_leaf': [10, 20, 30],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    # Create base model\n",
    "    gb_model = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=gb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Overall Accuracy (OA): {accuracy:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    # Ensure validation directory exists\n",
    "    os.makedirs(r'venv\\validation', exist_ok=True)\n",
    "    \n",
    "    # Save confusion matrix as CSV\n",
    "    unique_labels = sorted(np.unique(y_val))\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                         index=[f'True_{label}' for label in unique_labels],\n",
    "                         columns=[f'Pred_{label}' for label in unique_labels])\n",
    "    cm_df.to_csv(r'venv\\validation\\GBDT_weighted_confusion_matrix.csv')\n",
    "    \n",
    "    # Plot and save confusion matrix heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=unique_labels,\n",
    "                yticklabels=unique_labels)\n",
    "    plt.title('GBDT Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(r'venv\\validation\\GBDT_weighted_cm.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "def save_best_model(model, model_path):\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    # Save the model\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"\\nBest model saved to {model_path}\")\n",
    "\n",
    "def main():\n",
    "    # Dataset paths\n",
    "    weighted_train_path = r'venv\\dataset\\firstlayeroutput_weighted_probabilities_train.csv'\n",
    "    weighted_val_path = r'venv\\dataset\\firstlayeroutput_weighted_probabilities_val.csv'\n",
    "    \n",
    "    # Load weighted datasets\n",
    "    weighted_train, weighted_val = load_weighted_data(\n",
    "        weighted_train_path, weighted_val_path\n",
    "    )\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train_weighted, y_train_weighted, X_val_weighted, y_val_weighted = preprocess_data(\n",
    "        weighted_train, weighted_val\n",
    "    )\n",
    "    \n",
    "    # Perform grid search to find best parameters\n",
    "    print(\"Starting grid search...\")\n",
    "    grid_search = perform_grid_search(X_train_weighted, y_train_weighted)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(\"\\nBest Parameters Found:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    accuracy, f1 = evaluate_model(best_model, X_val_weighted, y_val_weighted)\n",
    "    \n",
    "    # Save the best model\n",
    "    model_path = r'venv\\models\\best_gbdt_model.pkl'\n",
    "    save_best_model(best_model, model_path)\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        'OA': accuracy,\n",
    "        'Accuracy': accuracy,  # OA and Accuracy are the same in this context\n",
    "        'F1': f1\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "    print(f\"Overall Accuracy (OA): {results['OA']:.4f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {results['F1']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce077d95-62bc-485a-b420-074f174ea2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tf",
   "language": "python",
   "name": "torch_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
