{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68204ca0-fc7c-4e47-b8d5-aa4e4cb49370",
   "metadata": {},
   "source": [
    "# Tutorial for Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d245fd-c49c-402f-af10-83b01a00c703",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<span style=\"color:rgb(224,142,134); font-family:Arial; font-size:15px\">This guide outlines the core workflow for training and validating the deep learning model. Ensure the runtime environment is properly configured according to the specifications in the model's documentation. Key parameters must be defined in the configuration file, including dataset paths, model hyperparameters, and training settings. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1673967-728d-4518-a76e-1de8fc5ee499",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Unet 3+ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae17e1-a9c2-479e-bdf2-5c24f80394f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from nets.UnetPlus3 import UNet3Plus as Unet\n",
    "from nets.unet_training import get_lr_scheduler, set_optimizer_lr, weights_init\n",
    "from utils.callbacks import LossHistory, EvalCallback\n",
    "from utils.dataloader import UnetDataset, unet_dataset_collate\n",
    "from utils.utils import download_weights, show_config\n",
    "from utils.utils_fit import fit_one_epoch\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from utils.utils_metrics import compute_mIoU, show_results\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd07efa-b05a-4e25-88e2-29370b71d596",
   "metadata": {},
   "source": [
    "### 1.1 training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31cc571-55e8-4a88-b19b-3901164a04b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before training, make sure to transform the dataset into VOC format.\n",
    "# \n",
    "if __name__ == \"__main__\":\n",
    "    Cuda = True\n",
    "    distributed     = False\n",
    "    sync_bn         = False\n",
    "    fp16            = False\n",
    "    num_classes = 8   #here num_classes = number of target categories + 1\n",
    "    backbone    = \"vgg\"\n",
    "    pretrained  = False\n",
    "    model_path  = \"[model path]\"\n",
    "    input_shape = [512, 512]\n",
    "    Init_Epoch          = 0\n",
    "    Freeze_Epoch        = 50\n",
    "    Freeze_batch_size   = 1\n",
    "    UnFreeze_Epoch      = 150  \n",
    "    Unfreeze_batch_size = 1\n",
    "    Freeze_Train        = False\n",
    "    Init_lr             = 1e-4   \n",
    "    Min_lr              = Init_lr * 0.01\n",
    "    optimizer_type      = \"adam\"  \n",
    "    momentum            = 0.9\n",
    "    weight_decay        = 0\n",
    "    lr_decay_type       = 'cos'\n",
    "    save_period         = 5\n",
    "    save_dir            = 'logs'\n",
    "    eval_flag           = True\n",
    "    eval_period         = 5\n",
    "    VOCdevkit_path  = 'VOCdevkit'\n",
    "    dice_loss       = True\n",
    "    focal_loss      = False\n",
    "    cls_weights     = np.ones([num_classes], np.float32)\n",
    "    num_workers     = 0\n",
    "    ngpus_per_node  = torch.cuda.device_count()\n",
    "    if distributed:\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "        local_rank  = int(os.environ[\"LOCAL_RANK\"])\n",
    "        rank        = int(os.environ[\"RANK\"])\n",
    "        device      = torch.device(\"cuda\", local_rank)\n",
    "        if local_rank == 0:\n",
    "            print(f\"[{os.getpid()}] (rank = {rank}, local_rank = {local_rank}) training...\")\n",
    "            print(\"Gpu Device Count : \", ngpus_per_node)\n",
    "    else:\n",
    "        device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        local_rank      = 0\n",
    "    if pretrained:\n",
    "        if distributed:\n",
    "            if local_rank == 0:\n",
    "                download_weights(backbone)  \n",
    "            dist.barrier()\n",
    "        else:\n",
    "            download_weights(backbone)\n",
    "    model = Unet(num_classes=num_classes, pretrained=pretrained, backbone=backbone).train()\n",
    "    if not pretrained:\n",
    "        weights_init(model)\n",
    "    if model_path != '':\n",
    "\n",
    "        if local_rank == 0:\n",
    "            print('Load weights {}.'.format(model_path))\n",
    "\n",
    "        model_dict      = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_path, map_location = device)\n",
    "        load_key, no_load_key, temp_dict = [], [], {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):\n",
    "                temp_dict[k] = v\n",
    "                load_key.append(k)\n",
    "            else:\n",
    "                no_load_key.append(k)\n",
    "        model_dict.update(temp_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        if local_rank == 0:\n",
    "            print(\"\\nSuccessful Load Key:\", str(load_key)[:500], \"……\\nSuccessful Load Key Num:\", len(load_key))\n",
    "            print(\"\\nFail To Load Key:\", str(no_load_key)[:500], \"……\\nFail To Load Key num:\", len(no_load_key))\n",
    "            print(\"\\n\\033[check head and Backbone\\033[0m\")\n",
    "\n",
    "    if local_rank == 0:\n",
    "        time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
    "        log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
    "        loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n",
    "    else:\n",
    "        loss_history    = None\n",
    "\n",
    "    if fp16:\n",
    "        from torch.cuda.amp import GradScaler as GradScaler\n",
    "        scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    model_train     = model.train()\n",
    "\n",
    "    if sync_bn and ngpus_per_node > 1 and distributed:\n",
    "        model_train = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_train)\n",
    "    elif sync_bn:\n",
    "        print(\"not support\")\n",
    "\n",
    "    if Cuda:\n",
    "        if distributed:\n",
    "\n",
    "            model_train = model_train.cuda(local_rank)\n",
    "            model_train = torch.nn.parallel.DistributedDataParallel(model_train, device_ids=[local_rank], find_unused_parameters=True)\n",
    "        else:\n",
    "            model_train = torch.nn.DataParallel(model)\n",
    "            cudnn.benchmark = True\n",
    "            model_train = model_train.cuda()\n",
    "\n",
    "    with open(os.path.join(VOCdevkit_path, \"[trainnum]\"),\"r\") as f:\n",
    "        train_lines = f.readlines()\n",
    "    with open(os.path.join(VOCdevkit_path, \"[valnum]\"),\"r\") as f:\n",
    "        val_lines = f.readlines()\n",
    "    num_train   = len(train_lines)\n",
    "    num_val     = len(val_lines)\n",
    "        \n",
    "    if local_rank == 0:\n",
    "        show_config(\n",
    "            num_classes = num_classes, backbone = backbone, model_path = model_path, input_shape = input_shape, \\\n",
    "            Init_Epoch = Init_Epoch, Freeze_Epoch = Freeze_Epoch, UnFreeze_Epoch = UnFreeze_Epoch, Freeze_batch_size = Freeze_batch_size, Unfreeze_batch_size = Unfreeze_batch_size, Freeze_Train = Freeze_Train, \\\n",
    "            Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n",
    "            save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n",
    "        )\n",
    "    \n",
    "    if True:\n",
    "        UnFreeze_flag = False\n",
    "\n",
    "        if Freeze_Train:\n",
    "            model.freeze_backbone()\n",
    "        batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n",
    "        nbs             = 16\n",
    "        lr_limit_max    = 1e-4 if optimizer_type == 'adam' else 1e-1\n",
    "        lr_limit_min    = 1e-4 if optimizer_type == 'adam' else 5e-4\n",
    "        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "        \n",
    "        optimizer = {\n",
    "            'adam'  : optim.Adam(model.parameters(), Init_lr_fit, betas = (momentum, 0.999), weight_decay = weight_decay),\n",
    "            'sgd'   : optim.SGD(model.parameters(), Init_lr_fit, momentum = momentum, nesterov=True, weight_decay = weight_decay)\n",
    "        }[optimizer_type]\n",
    "        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "        epoch_step      = num_train // batch_size\n",
    "        epoch_step_val  = num_val // batch_size\n",
    "        \n",
    "        if epoch_step == 0 or epoch_step_val == 0:\n",
    "            raise ValueError(\"size emergency\")\n",
    "\n",
    "        train_dataset   = UnetDataset(train_lines, input_shape, num_classes, True, VOCdevkit_path)\n",
    "        val_dataset     = UnetDataset(val_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "        \n",
    "        if distributed:\n",
    "            train_sampler   = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True,)\n",
    "            val_sampler     = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False,)\n",
    "            batch_size      = batch_size // ngpus_per_node\n",
    "            shuffle         = False\n",
    "        else:\n",
    "            train_sampler   = None\n",
    "            val_sampler     = None\n",
    "            shuffle         = True\n",
    "\n",
    "        gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
    "                                    drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "        gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n",
    "                                    drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "        if local_rank == 0:\n",
    "            eval_callback   = EvalCallback(model, input_shape, num_classes, val_lines, VOCdevkit_path, log_dir, Cuda, \\\n",
    "                                            eval_flag=eval_flag, period=eval_period)\n",
    "        else:\n",
    "            eval_callback   = None\n",
    "\n",
    "        for epoch in range(Init_Epoch, UnFreeze_Epoch):\n",
    "\n",
    "            if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train:\n",
    "                batch_size = Unfreeze_batch_size\n",
    "                nbs             = 16\n",
    "                lr_limit_max    = 1e-4 if optimizer_type == 'adam' else 1e-1\n",
    "                lr_limit_min    = 1e-4 if optimizer_type == 'adam' else 5e-4\n",
    "                Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "                Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "\n",
    "                lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "                model.unfreeze_backbone()\n",
    "                epoch_step      = num_train // batch_size\n",
    "                epoch_step_val  = num_val // batch_size\n",
    "                if epoch_step == 0 or epoch_step_val == 0:\n",
    "                    raise ValueError(\"size emergency\")\n",
    "                if distributed:\n",
    "                    batch_size = batch_size // ngpus_per_node\n",
    "\n",
    "                gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
    "                                            drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "                gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n",
    "                                            drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "\n",
    "                UnFreeze_flag = True\n",
    "\n",
    "            if distributed:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "\n",
    "            set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
    "\n",
    "            fit_one_epoch(model_train, model, loss_history, eval_callback, optimizer, epoch, \n",
    "                    epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank)\n",
    "\n",
    "            if distributed:\n",
    "                dist.barrier()\n",
    "\n",
    "        if local_rank == 0:\n",
    "            loss_history.writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56350b2-bedf-4eeb-ac4b-b45b764da381",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca1c0d-5231-474c-b2ff-4ec3ff12e897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Configuration Notes:\n",
    "#  Default parameters are optimized for tree species classification. \n",
    "#  For shadow detection tasks, please modify:\n",
    "#    - num_classes = 2                      # background + shadow\n",
    "#    - name_classes = [\"_background_\", \"Shadow\"]\n",
    "#  Note: Model's final layer output dimension must match the new num_classes.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    miou_mode       = 0\n",
    "    num_classes     = 8\n",
    "    name_classes = [\"_background_\",\"Platanus\",\"Cinnamomum\",\"Ginkgo\",\"Sapindaceae\",\"Cedrus\",\"Salix\",\"Metasequoia\"] # here type the name of each class\n",
    "    VOCdevkit_path  = 'voc_path'\n",
    "    image_ids       = open(os.path.join(VOCdevkit_path, \"val.txt\"),'r').read().splitlines()\n",
    "    gt_dir          = os.path.join(VOCdevkit_path, \"[set_data_path]\")\n",
    "    miou_out_path   = \"miou_out\"\n",
    "    pred_dir        = os.path.join(miou_out_path, 'detection-results')\n",
    "\n",
    "    if miou_mode == 0 or miou_mode == 1:\n",
    "        if not os.path.exists(pred_dir):\n",
    "            os.makedirs(pred_dir)\n",
    "            \n",
    "        print(\"Load model.\")\n",
    "        unet = Unet()\n",
    "        print(\"Load model done.\")\n",
    "\n",
    "        print(\"Get predict result.\")\n",
    "        for image_id in tqdm(image_ids):\n",
    "            image_path  = os.path.join(VOCdevkit_path, \"[dateset_path]\"+image_id+\".png\")\n",
    "            image       = Image.open(image_path)\n",
    "            image       = unet.get_miou_png(image)\n",
    "            image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "        print(\"Get predict result done.\")\n",
    "\n",
    "    if miou_mode == 0 or miou_mode == 2:\n",
    "        print(\"Get miou.\")\n",
    "        hist, IoUs, PA_Recall, Precision = compute_mIoU(gt_dir, pred_dir, image_ids, num_classes, name_classes)  \n",
    "        print(\"Get miou done.\")\n",
    "        show_results(miou_out_path, hist, IoUs, PA_Recall, Precision, name_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8edb4-13c9-45bd-a7e7-0b6da6a20366",
   "metadata": {},
   "source": [
    "### 1.3 generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1400a-b92b-4694-9c81-c07c2630b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    unet = Unet()\n",
    "    mode = \"dir_predict\"\n",
    "    count           = False\n",
    "    name_classes    = [\"_background_\",\"Platanus\",\"Cinnamomum\",\"Ginkgo\",\"Sapindaceae\",\"Cedrus\",\"Salix\",\"Metasequoia\"]\n",
    "    video_path      = 0\n",
    "    video_save_path = \"\"\n",
    "    video_fps       = 25.0\n",
    "    test_interval = 100\n",
    "    fps_image_path  = \"img/fps\"\n",
    "    dir_origin_path = \"img/\"\n",
    "    dir_save_path   = \"img_out/\"\n",
    "    simplify        = True\n",
    "    onnx_save_path  = \"model_data/models.onnx\"\n",
    "    if mode == \"predict\":\n",
    "        while True:\n",
    "            img = input('Input image filename:')\n",
    "            try:\n",
    "                image = Image.open(img)\n",
    "            except:\n",
    "                print('Open Error! Try again!')\n",
    "                continue\n",
    "            else:\n",
    "                r_image = unet.detect_image(image, count=count, name_classes=name_classes)\n",
    "                r_image.show()\n",
    "    elif mode == \"video\":\n",
    "        capture=cv2.VideoCapture(video_path)\n",
    "        if video_save_path!=\"\":\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "            out = cv2.VideoWriter(video_save_path, fourcc, video_fps, size)\n",
    "\n",
    "        ref, frame = capture.read()\n",
    "        if not ref:\n",
    "            raise ValueError(\"\")\n",
    "\n",
    "        fps = 0.0\n",
    "        while(True):\n",
    "            t1 = time.time()\n",
    "            ref, frame = capture.read()\n",
    "            if not ref:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(np.uint8(frame))\n",
    "            frame = np.array(unet.detect_image(frame))\n",
    "            frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
    "            fps  = ( fps + (1./(time.time()-t1)) ) / 2\n",
    "            print(\"fps= %.2f\"%(fps))\n",
    "            frame = cv2.putText(frame, \"fps= %.2f\"%(fps), (0, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow(\"video\",frame)\n",
    "            c= cv2.waitKey(1) & 0xff \n",
    "            if video_save_path!=\"\":\n",
    "                out.write(frame)\n",
    "            if c==27:\n",
    "                capture.release()\n",
    "                break\n",
    "        print(\"Video Detection Done!\")\n",
    "        capture.release()\n",
    "        if video_save_path!=\"\":\n",
    "            print(\"Save processed video to the path :\" + video_save_path)\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    elif mode == \"fps\":\n",
    "        img = Image.open('img/street.jpg')\n",
    "        tact_time = unet.get_FPS(img, test_interval)\n",
    "        print(str(tact_time) + ' seconds, ' + str(1/tact_time) + 'FPS, @batch_size 1')\n",
    "        \n",
    "    elif mode == \"dir_predict\":\n",
    "        import os\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        img_names = os.listdir(dir_origin_path)\n",
    "        for img_name in tqdm(img_names):\n",
    "            if img_name.lower().endswith(('.bmp', '.dib', '.png', '.jpg', '.jpeg', '.pbm', '.pgm', '.ppm', '.tif', '.tiff')):\n",
    "                image_path  = os.path.join(dir_origin_path, img_name)\n",
    "                image       = Image.open(image_path)\n",
    "                r_image     = unet.detect_image(image)\n",
    "                if not os.path.exists(dir_save_path):\n",
    "                    os.makedirs(dir_save_path)\n",
    "                r_image.save(os.path.join(dir_save_path, img_name))\n",
    "    elif mode == \"export_onnx\":\n",
    "        unet.convert_to_onnx(simplify, onnx_save_path)\n",
    "                \n",
    "    else:\n",
    "        raise AssertionError(\"Please specify the correct mode: 'predict', 'video', 'fps' or 'dir_predict'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23b7b8-1702-4ed2-95d3-649277f19a9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. DeepLabv3+ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125aa7d2-bd0e-45f5-a43e-fa405008adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "from mmengine.config import Config, DictAction\n",
    "from mmengine.logging import print_log\n",
    "from mmengine.runner import Runner\n",
    "from mmseg.registry import RUNNERS\n",
    "from argparse import ArgumentParser\n",
    "from mmengine.model import revert_sync_batchnorm\n",
    "from mmseg.apis import inference_model, init_model, show_result_pyplot\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7b75b-8002-46f3-82d5-25b8dc467704",
   "metadata": {},
   "source": [
    "### 2.1 training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db478f43-e291-43b9-a06f-461cd02fd21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before training, make sure to transform the dataset into VOC format.\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a segmentor')\n",
    "    parser.add_argument('--config',default='deeplabv3plus_r50-d8_4xb4-40k_voc12aug-512x512.py')\n",
    "    parser.add_argument('--work-dir')\n",
    "    parser.add_argument(\n",
    "        '--resume',\n",
    "        action='store_true',\n",
    "        default=False)\n",
    "    parser.add_argument(\n",
    "        '--amp',\n",
    "        action='store_true',\n",
    "        default=False')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction)\n",
    "    parser.add_argument(\n",
    "        '--launcher',\n",
    "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
    "        default='none')\n",
    "    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "    return args\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    cfg.launcher = args.launcher\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "    if args.work_dir is not None:\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        cfg.work_dir = osp.join('./work_dirs',\n",
    "                                osp.splitext(osp.basename(args.config))[0])\n",
    "\n",
    "    if args.amp is True:\n",
    "        optim_wrapper = cfg.optim_wrapper.type\n",
    "        if optim_wrapper == 'AmpOptimWrapper':\n",
    "            print_log(\n",
    "                'AMP training is already enabled in your config.',\n",
    "                logger='current',\n",
    "                level=logging.WARNING)\n",
    "        else:\n",
    "            assert optim_wrapper == 'OptimWrapper', (\n",
    "                '`--amp` is only supported when the optimizer wrapper type is '\n",
    "                f'`OptimWrapper` but got {optim_wrapper}.')\n",
    "            cfg.optim_wrapper.type = 'AmpOptimWrapper'\n",
    "            cfg.optim_wrapper.loss_scale = 'dynamic'\n",
    "\n",
    "    cfg.resume = args.resume\n",
    "\n",
    "    if 'runner_type' not in cfg:\n",
    "        runner = Runner.from_cfg(cfg)\n",
    "    else:\n",
    "        runner = RUNNERS.build(cfg)\n",
    "\n",
    "    runner.train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2694f3a-dd98-4bfe-b1bd-c92874a1f9db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b33b49-d0fe-419a-9a35-14690531600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_labels = False\n",
    "title = 'result'\n",
    "opacity = 0.5\n",
    "imagepath = r''\n",
    "savepath = r''\n",
    "config_file = r''\n",
    "checkpoint_file = r''\n",
    "device = 'cpu'\n",
    "model = init_model(config_file, checkpoint_file, device=device)\n",
    "if  device == 'cpu':\n",
    "    model = revert_sync_batchnorm(model)\n",
    "for filename in os.listdir(imagepath):\n",
    "    print(\"testing：\", filename)\n",
    "    img = os.path.join(imagepath, filename)\n",
    "    result = inference_model(model, img)\n",
    "    out_file = os.path.join(savepath, filename)\n",
    "    show_result_pyplot(\n",
    "        model,\n",
    "        img,\n",
    "        result,\n",
    "        title=title,\n",
    "        opacity=opacity,\n",
    "        with_labels=with_labels,\n",
    "        draw_gt=False,\n",
    "        show=False if out_file is not None else True,\n",
    "        out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49baf1-59c5-4984-9538-13f97c1b4dc9",
   "metadata": {},
   "source": [
    "##  3. Mask R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6c9cc-b901-40bc-9c0b-b88deba222b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "from mmengine.config import Config, DictAction\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmengine.runner import Runner\n",
    "from mmdet.utils import setup_cache_size_limit_of_dynamo\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mmcv.ops import nms\n",
    "from mmengine import Config, DictAction\n",
    "from mmengine.fileio import load\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmengine.utils import ProgressBar\n",
    "from mmdet.evaluation import bbox_overlaps\n",
    "from mmdet.registry import DATASETS\n",
    "from mmdet.utils import replace_cfg_vals, update_data_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dac725-9e69-4b6a-944c-7a8a19701ad3",
   "metadata": {},
   "source": [
    "### 3.1 training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dd61c-f492-4144-a239-b6e2a98f3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, make sure to transform the dataset into COCO format.\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a detector')\n",
    "    parser.add_argument('--config', default='configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py')\n",
    "    parser.add_argument('--work-dir')\n",
    "    parser.add_argument(\n",
    "        '--amp',\n",
    "        action='store_true',\n",
    "        default=False)\n",
    "    parser.add_argument(\n",
    "        '--auto-scale-lr',\n",
    "        action='store_true')\n",
    "    parser.add_argument(\n",
    "        '--resume',\n",
    "        nargs='?',\n",
    "        type=str,\n",
    "        const='auto')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction)\n",
    "    parser.add_argument(\n",
    "        '--launcher',\n",
    "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
    "        default='none')\n",
    "    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    setup_cache_size_limit_of_dynamo()\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    cfg.launcher = args.launcher\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "    if args.work_dir is not None:\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        cfg.work_dir = osp.join('./work_dirs',\n",
    "                                osp.splitext(osp.basename(args.config))[0])\n",
    "\n",
    "    if args.amp is True:\n",
    "        cfg.optim_wrapper.type = 'AmpOptimWrapper'\n",
    "        cfg.optim_wrapper.loss_scale = 'dynamic'\n",
    "\n",
    "    if args.auto_scale_lr:\n",
    "        if 'auto_scale_lr' in cfg and \\\n",
    "                'enable' in cfg.auto_scale_lr and \\\n",
    "                'base_batch_size' in cfg.auto_scale_lr:\n",
    "            cfg.auto_scale_lr.enable = True\n",
    "        else:\n",
    "            raise RuntimeError('Can not find \"auto_scale_lr\" or '\n",
    "                               '\"auto_scale_lr.enable\" or '\n",
    "                               '\"auto_scale_lr.base_batch_size\" in your'\n",
    "                               ' configuration file.')\n",
    "\n",
    "    if args.resume == 'auto':\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = None\n",
    "    elif args.resume is not None:\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = args.resume\n",
    "\n",
    "    if 'runner_type' not in cfg:\n",
    "        runner = Runner.from_cfg(cfg)\n",
    "    else:\n",
    "        runner = RUNNERS.build(cfg)\n",
    "\n",
    "    runner.train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ed669-39de-4f20-89f3-86c293cbafac",
   "metadata": {},
   "source": [
    "### 3.2 validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c11d80-dd6d-429d-bef7-854948a800fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Generate confusion matrix from detection results')\n",
    "    parser.add_argument('--config', default='mask-rcnn_r50_fpn_1x_coco.py')\n",
    "    parser.add_argument(\n",
    "        '--prediction_path',default='maskrcnn.pkl')\n",
    "    parser.add_argument(\n",
    "        '--save_dir',\n",
    "        default='./')\n",
    "    parser.add_argument(\n",
    "        '--show', action='store_true')\n",
    "    parser.add_argument(\n",
    "        '--color-theme',\n",
    "        default='plasma')\n",
    "    parser.add_argument(\n",
    "        '--score-thr',\n",
    "        type=float,\n",
    "        default=0.3)\n",
    "    parser.add_argument(\n",
    "        '--tp-iou-thr',\n",
    "        type=float,\n",
    "        default=0.5)\n",
    "    parser.add_argument(\n",
    "        '--nms-iou-thr',\n",
    "        type=float,\n",
    "        default=None)\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(dataset,\n",
    "                               results,\n",
    "                               score_thr=0,\n",
    "                               nms_iou_thr=None,\n",
    "                               tp_iou_thr=0.5):\n",
    "\n",
    "    num_classes = len(dataset.metainfo['classes'])\n",
    "    confusion_matrix = np.zeros(shape=[num_classes + 1, num_classes + 1])\n",
    "    assert len(dataset) == len(results)\n",
    "    prog_bar = ProgressBar(len(results))\n",
    "    for idx, per_img_res in enumerate(results):\n",
    "        res_bboxes = per_img_res['pred_instances']\n",
    "        gts = dataset.get_data_info(idx)['instances']\n",
    "        analyze_per_img_dets(confusion_matrix, gts, res_bboxes, score_thr,\n",
    "                             tp_iou_thr, nms_iou_thr)\n",
    "        prog_bar.update()\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def analyze_per_img_dets(confusion_matrix,\n",
    "                         gts,\n",
    "                         result,\n",
    "                         score_thr=0,\n",
    "                         tp_iou_thr=0.5,\n",
    "                         nms_iou_thr=None):\n",
    "\n",
    "    true_positives = np.zeros(len(gts))\n",
    "    gt_bboxes = []\n",
    "    gt_labels = []\n",
    "    for gt in gts:\n",
    "        gt_bboxes.append(gt['bbox'])\n",
    "        gt_labels.append(gt['bbox_label'])\n",
    "\n",
    "    gt_bboxes = np.array(gt_bboxes)\n",
    "    gt_labels = np.array(gt_labels)\n",
    "\n",
    "    unique_label = np.unique(result['labels'].numpy())\n",
    "\n",
    "    for det_label in unique_label:\n",
    "        mask = (result['labels'] == det_label)\n",
    "        det_bboxes = result['bboxes'][mask].numpy()\n",
    "        det_scores = result['scores'][mask].numpy()\n",
    "\n",
    "        if nms_iou_thr:\n",
    "            det_bboxes, _ = nms(\n",
    "                det_bboxes, det_scores, nms_iou_thr, score_threshold=score_thr)\n",
    "        ious = bbox_overlaps(det_bboxes[:, :4], gt_bboxes)\n",
    "        for i, score in enumerate(det_scores):\n",
    "            det_match = 0\n",
    "            if score >= score_thr:\n",
    "                for j, gt_label in enumerate(gt_labels):\n",
    "                    if ious[i, j] >= tp_iou_thr:\n",
    "                        det_match += 1\n",
    "                        if gt_label == det_label:\n",
    "                            true_positives[j] += 1\n",
    "                        confusion_matrix[gt_label, det_label] += 1\n",
    "                if det_match == 0:\n",
    "                    confusion_matrix[-1, det_label] += 1\n",
    "    for num_tp, gt_label in zip(true_positives, gt_labels):\n",
    "        if num_tp == 0:\n",
    "            confusion_matrix[gt_label, -1] += 1\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix,\n",
    "                          labels,\n",
    "                          save_dir=None,\n",
    "                          show=True,\n",
    "                          title='Normalized Confusion Matrix',\n",
    "                          color_theme='plasma'):\n",
    "\n",
    "    per_label_sums = confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    confusion_matrix = \\\n",
    "        confusion_matrix.astype(np.float32) / per_label_sums * 100\n",
    "\n",
    "    num_classes = len(labels)\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(0.5 * num_classes, 0.5 * num_classes * 0.8), dpi=180)\n",
    "    cmap = plt.get_cmap(color_theme)\n",
    "    im = ax.imshow(confusion_matrix, cmap=cmap)\n",
    "    plt.colorbar(mappable=im, ax=ax)\n",
    "\n",
    "    title_font = {'weight': 'bold', 'size': 12}\n",
    "    ax.set_title(title, fontdict=title_font)\n",
    "    label_font = {'size': 10}\n",
    "    plt.ylabel('Ground Truth Label', fontdict=label_font)\n",
    "    plt.xlabel('Prediction Label', fontdict=label_font)\n",
    "\n",
    "    xmajor_locator = MultipleLocator(1)\n",
    "    xminor_locator = MultipleLocator(0.5)\n",
    "    ax.xaxis.set_major_locator(xmajor_locator)\n",
    "    ax.xaxis.set_minor_locator(xminor_locator)\n",
    "    ymajor_locator = MultipleLocator(1)\n",
    "    yminor_locator = MultipleLocator(0.5)\n",
    "    ax.yaxis.set_major_locator(ymajor_locator)\n",
    "    ax.yaxis.set_minor_locator(yminor_locator)\n",
    "\n",
    "    ax.grid(True, which='minor', linestyle='-')\n",
    "\n",
    "    ax.set_xticks(np.arange(num_classes))\n",
    "    ax.set_yticks(np.arange(num_classes))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    ax.tick_params(\n",
    "        axis='x', bottom=False, top=True, labelbottom=False, labeltop=True)\n",
    "    plt.setp(\n",
    "        ax.get_xticklabels(), rotation=45, ha='left', rotation_mode='anchor')\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                '{}%'.format(\n",
    "                    int(confusion_matrix[\n",
    "                        i,\n",
    "                        j]) if not np.isnan(confusion_matrix[i, j]) else -1),\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                color='w',\n",
    "                size=7)\n",
    "\n",
    "    ax.set_ylim(len(confusion_matrix) - 0.5, -0.5)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_dir is not None:\n",
    "        plt.savefig(\n",
    "            os.path.join(save_dir, 'confusion_matrix.png'), format='png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    cfg = Config.fromfile(args.config)\n",
    "\n",
    "    cfg = replace_cfg_vals(cfg)\n",
    "\n",
    "    update_data_root(cfg)\n",
    "\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    init_default_scope(cfg.get('default_scope', 'mmdet'))\n",
    "\n",
    "    results = load(args.prediction_path)\n",
    "\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    dataset = DATASETS.build(cfg.test_dataloader.dataset)\n",
    "\n",
    "    confusion_matrix = calculate_confusion_matrix(dataset, results,\n",
    "                                                  args.score_thr,\n",
    "                                                  args.nms_iou_thr,\n",
    "                                                  args.tp_iou_thr)\n",
    "    plot_confusion_matrix(\n",
    "        confusion_matrix,\n",
    "        dataset.metainfo['classes'] + ('background', ),\n",
    "        save_dir=args.save_dir,\n",
    "        show=args.show,\n",
    "        color_theme=args.color_theme)\n",
    "\n",
    "\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    FP = np.sum(confusion_matrix, axis=0) - TP\n",
    "    FN = np.sum(confusion_matrix, axis=1) - TP\n",
    "\n",
    "    recall = TP / (TP + FP)\n",
    "    precision = TP / (TP + FN)\n",
    "    f1 =  2 * (precision * recall) / (precision + recall)\n",
    "    average_precision = sum(precision) / 7\n",
    "    average_recall = sum(recall) / 7\n",
    "\n",
    "    average_f1 = 2 * (average_precision * average_recall) / (average_precision + average_recall)\n",
    "    accuracy = np.sum(TP) / np.sum(confusion_matrix)\n",
    "\n",
    "    print('/n')\n",
    "    print(dataset.metainfo['classes'] + ('background', ))\n",
    "\n",
    "    print(\" precision \", precision)\n",
    "    print(\"recall \", recall)\n",
    "    print(\"f1 \", f1)\n",
    "\n",
    "    print(\"average P \", average_precision)\n",
    "    print(\"average R\", average_recall)\n",
    "    print(\"average F1\", average_f1)\n",
    "    print(\"accuracy \", accuracy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
